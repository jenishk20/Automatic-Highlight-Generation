{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-13T00:10:33.160853Z",
     "iopub.status.busy": "2025-04-13T00:10:33.160586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, cv2, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    torch.backends.cudnn.benchmark = True  # speed for fixed‑size inputs\n",
    "\n",
    "\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    \"\"\"ResNet‑34 backbone (output = 512‑D vector).\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])  # cut off FC\n",
    "\n",
    "    def forward(self, x):                          # (N,3,H,W)\n",
    "        x = self.backbone(x)                       # (N,512,1,1)\n",
    "        return x.flatten(1)                        # (N,512)\n",
    "\n",
    "class LSTMWithResNet(nn.Module):\n",
    "    \"\"\"Frame‑level CNN + sequence‑level LSTM classifier.\"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_size: int,\n",
    "                 hidden_size:  int,\n",
    "                 output_size:  int,\n",
    "                 num_layers:   int = 3,\n",
    "                 dropout:      float = 0.3):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = ResNetFeatureExtractor()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = feature_size,\n",
    "            hidden_size  = hidden_size,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size * 2, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, clip):                       # (B,T,3,H,W)\n",
    "        B, T, C, H, W = clip.shape\n",
    "        clip = clip.view(-1, C, H, W)              # (B*T,3,H,W)\n",
    "        feats = self.feature_extractor(clip)       # (B*T,512)\n",
    "        feats = feats.view(B, T, -1)               # (B,T,512)\n",
    "        seq_out, _ = self.lstm(feats)              # (B,T,H)\n",
    "        last = seq_out[:, -1, :]                   # (B,H)\n",
    "        return self.fc(last)                       # (B,output)\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, label_to_index, resize=224, transform=None):\n",
    "        self.video_paths, self.labels = video_paths, labels\n",
    "        self.label_to_index, self.transform = label_to_index, transform\n",
    "        self.resize = resize\n",
    "\n",
    "    def __len__(self):  return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.video_paths[idx], self.labels[idx]\n",
    "        frames, cap = [], cv2.VideoCapture(path)\n",
    "        while cap.isOpened():\n",
    "            ok, frame = cap.read()\n",
    "            if not ok: break\n",
    "            frame = cv2.resize(frame, (self.resize, self.resize)) / 255.0\n",
    "            if self.transform: frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "\n",
    "        clip = torch.tensor(np.stack(frames)).permute(0,3,1,2).float()  # (T,3,H,W)\n",
    "        return clip, self.label_to_index[label]\n",
    "\n",
    "\n",
    "def load_data(root_dir, exts=(\".mp4\", \".avi\", \".mov\", \".mkv\")):\n",
    "    \"\"\"Recursively collect video paths and labels.\n",
    "    Assumes folder structure:  root_dir/<label>/<video files>.\"\"\"\n",
    "    paths, lbls = [], []\n",
    "    for lbl in os.listdir(root_dir):\n",
    "        p = os.path.join(root_dir, lbl)\n",
    "        if not os.path.isdir(p):\n",
    "            continue\n",
    "        for f in os.listdir(p):\n",
    "            if f.lower().endswith(tuple(e.lower() for e in exts)):\n",
    "                paths.append(os.path.join(p, f))\n",
    "                lbls.append(lbl)\n",
    "    return paths, lbls\n",
    "\n",
    "root_dir     = \"/kaggle/input/match-highlight-extracted/extracted\"  \n",
    "frame_size   = 160            \n",
    "feature_size = 512\n",
    "hidden_size  = 512\n",
    "num_layers   = 3\n",
    "dropout      = 0.3\n",
    "epochs       = 20\n",
    "batch_size   = 1              \n",
    "accum_steps  = 4              \n",
    "lr           = 1e-4\n",
    "\n",
    "# ---- checkpoint settings ----\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "save_every = 2          # save a full checkpoint every N epochs\n",
    "best_acc = 0.0          # track best validation accuracy\n",
    "\n",
    "\n",
    "paths, labels = load_data(root_dir)\n",
    "if len(paths) == 0:\n",
    "    raise ValueError(f\"No video files found under '{root_dir}'. Check path/structure/extensions.\")\n",
    "\n",
    "unique = sorted(set(labels))\n",
    "label_to_index = {l:i for i,l in enumerate(unique)}\n",
    "index_to_label = {i:l for l,i in label_to_index.items()}\n",
    "\n",
    "tr_p, val_p, tr_l, val_l = train_test_split(paths, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "train_ds = VideoDataset(tr_p, tr_l, label_to_index, resize=frame_size)\n",
    "val_ds   = VideoDataset(val_p, val_l, label_to_index, resize=frame_size)\n",
    "train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_ld   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "model = LSTMWithResNet(feature_size, hidden_size, len(unique), num_layers, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# mixed‑precision support\n",
    "amp_enabled = device.type == \"cuda\"\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=amp_enabled)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train(); running = 0.0\n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, (clips, lbl) in enumerate(tqdm(train_ld, desc=f\"Epoch {epoch}/{epochs}\")):\n",
    "        clips, lbl = clips.to(device, non_blocking=True), lbl.to(device, non_blocking=True)\n",
    "        try:\n",
    "            with torch.cuda.amp.autocast(enabled=amp_enabled):\n",
    "                logits = model(clips)\n",
    "                loss = criterion(logits, lbl) / accum_steps\n",
    "            scaler.scale(loss).backward()\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(\"⚠️  OOM at step\", step, \"– skipping batch\")\n",
    "                torch.cuda.empty_cache(); continue\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "        if (step + 1) % accum_steps == 0 or (step + 1) == len(train_ld):\n",
    "            scaler.step(optimiser)\n",
    "            scaler.update()\n",
    "            optimiser.zero_grad(set_to_none=True)\n",
    "        running += loss.item() * accum_steps  # undo division for logging\n",
    "\n",
    "    train_loss = running / len(train_ld)\n",
    "    print(f\"  Train loss: {train_loss:.4f}\")\n",
    "\n",
    "    # ---- validation ----\n",
    "    model.eval(); correct = tot = 0\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=amp_enabled):\n",
    "        for clips, lbl in val_ld:\n",
    "            clips, lbl = clips.to(device, non_blocking=True), lbl.to(device, non_blocking=True)\n",
    "            preds = model(clips).argmax(1)\n",
    "            correct += (preds == lbl).sum().item()\n",
    "            tot += lbl.size(0)\n",
    "    val_acc = correct / tot\n",
    "    print(f\"  Val  acc : {val_acc:.4f}\")\n",
    "\n",
    "    # ---- checkpointing ----\n",
    "    if epoch % save_every == 0:\n",
    "        ckpt_path = os.path.join(checkpoint_dir, f\"epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimiser.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'train_loss': train_loss\n",
    "        }, ckpt_path)\n",
    "        print(f\"  ✓ Checkpoint saved: {ckpt_path}\")\n",
    "\n",
    "    global best_acc\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_path = os.path.join(checkpoint_dir, \"best.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimiser.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'train_loss': train_loss\n",
    "        }, best_path)\n",
    "        print(f\"  ✓ New best model ({best_acc:.4f}) saved to {best_path}\")\n",
    "\n",
    "# save final model\n",
    "final_path = os.path.join(checkpoint_dir, \"final.pth\")\n",
    "torch.save(model.state_dict(), final_path)\n",
    "print(f\"✓ Training finished & final model saved to {final_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, test_dir):\n",
    "    import matplotlib.pyplot as plt, seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "    test_paths, test_labels = load_data(test_dir)\n",
    "    lbl2idx = {l:i for i,l in enumerate(sorted(set(test_labels)))}\n",
    "    idx2lbl = {i:l for l,i in lbl2idx.items()}\n",
    "    test_ds = VideoDataset(test_paths, test_labels, lbl2idx)\n",
    "    test_ld = DataLoader(test_ds, batch_size=2, shuffle=False)\n",
    "\n",
    "    # determine number of classes from checkpoint if possible\n",
    "    ckpt = torch.load(model_path, map_location=device)\n",
    "    n_classes = ckpt['model_state_dict']['fc.3.weight'].shape[0] if isinstance(ckpt, dict) else len(lbl2idx)\n",
    "\n",
    "    net = LSTMWithResNet(feature_size, hidden_size, n_classes).to(device)\n",
    "    if isinstance(ckpt, dict):\n",
    "        net.load_state_dict(ckpt['model_state_dict'])\n",
    "    else:\n",
    "        net.load_state_dict(ckpt)\n",
    "    net.eval()\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for clips, lbl in test_ld:\n",
    "            clips, lbl = clips.to(device), lbl.to(device)\n",
    "            y_true.extend(lbl.cpu().numpy())\n",
    "            y_pred.extend(net(clips).argmax(1).cpu().numpy())\n",
    "\n",
    "    print(\"\\nAccuracy:\", (np.array(y_true) == np.array(y_pred)).mean())\n",
    "    print(\"\\nReport:\\n\", classification_report(y_true, y_pred, target_names=[idx2lbl[i] for i in idx2lbl]))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[idx2lbl[i] for i in idx2lbl],\n",
    "                yticklabels=[idx2lbl[i] for i in idx2lbl])\n",
    "    plt.xlabel(\"Pred\"); plt.ylabel(\"True\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "def predict_video(video_path, model_path):\n",
    "    ckpt = torch.load(model_path, map_location=device)\n",
    "    n_classes = ckpt['model_state_dict']['fc.3.weight'].shape[0] if isinstance(ckpt, dict) else len(unique)\n",
    "\n",
    "    net = LSTMWithResNet(feature_size, hidden_size, n_classes).to(device)\n",
    "    if isinstance(ckpt, dict):\n",
    "        net.load_state_dict(ckpt['model_state_dict'])\n",
    "    else:\n",
    "        net.load_state_dict(ckpt)\n",
    "    net.eval()\n",
    "\n",
    "    frames, cap = [], cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ok, f = cap.read()\n",
    "        if not ok: break\n",
    "        f = cv2.resize(f, (224,224)) / 255.0\n",
    "        frames.append(f)\n",
    "    cap.release()\n",
    "\n",
    "    clip = torch.tensor(np.stack(frames)).permute(0,3,1,2).float().unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        probs = torch.softmax(net(clip), 1)[0]\n",
    "    idx = probs.argmax().item()\n",
    "    print(f\"Predicted: {index_to_label.get(idx, idx)}  (conf {probs[idx]:.2f})\")\n",
    "    return index_to_label.get(idx, idx), probs.cpu().numpy()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7127200,
     "sourceId": 11382528,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
