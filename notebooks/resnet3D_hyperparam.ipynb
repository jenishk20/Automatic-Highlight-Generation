{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.models.video as video_models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'data_root': '../data/9-classes',\n",
    "    'frame_height': 224,\n",
    "    'frame_width': 224,\n",
    "    'num_frames': 32,\n",
    "\n",
    "    'batch_size': 4,\n",
    "    'epochs': 1,\n",
    "    'learning_rate': 1e-4,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'fold' : 2,\n",
    "    'optimizer': 'adam',\n",
    "    'scheduler': 'cosine',\n",
    "    'dropout': 0.5,\n",
    "\n",
    "    'model_type': 'r3d',\n",
    "    'pretrained': True,\n",
    "}\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_paths(data_root):\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(data_root))\n",
    "    label_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "    for class_name in class_names:\n",
    "        class_dir = Path(data_root) / class_name\n",
    "        for video_file in class_dir.glob(\"*.mp4\"):\n",
    "            video_paths.append(str(video_file))\n",
    "            labels.append(label_to_idx[class_name])\n",
    "    return video_paths, labels, label_to_idx\n",
    "\n",
    "video_paths, labels, label_to_idx = load_video_paths(CONFIG['data_root'])\n",
    "idx_to_label = {v: k for k, v in label_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoccerDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, config):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.config = config\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(height=config['frame_height'], width=config['frame_width']),\n",
    "            A.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        frames = self._load_video(path)\n",
    "        return frames, label\n",
    "\n",
    "    def _load_video(self, path):\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        indices = np.linspace(0, frame_count - 1, self.config['num_frames'], dtype=int)\n",
    "        frames = []\n",
    "\n",
    "        for i in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = self.transform(image=frame)['image']\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        frames = np.array(frames)\n",
    "        frames = np.transpose(frames, (3, 0, 1, 2))\n",
    "        return torch.from_numpy(frames).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R3DClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(R3DClassifier, self).__init__()\n",
    "        self.model = video_models.r3d_18(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = R3DClassifier(num_classes=len(label_to_idx)).to(CONFIG['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "‚ñ∂Ô∏è Fold 1/2\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenishkothari/Northeastern/CS6140/Automatic Highlight Generation/myenv/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/jenishkothari/Northeastern/CS6140/Automatic Highlight Generation/myenv/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Fold 1 - Epoch 1:   1%|          | 1/89 [01:23<2:02:09, 83.29s/it]"
     ]
    }
   ],
   "source": [
    "k_folds = CONFIG['fold']\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "all_fold_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(video_paths, labels)):\n",
    "    print(f\"\\n{'='*30}\\n‚ñ∂Ô∏è Fold {fold+1}/{k_folds}\\n{'='*30}\")\n",
    "\n",
    "    train_paths_fold = [video_paths[i] for i in train_idx]\n",
    "    val_paths_fold = [video_paths[i] for i in val_idx]\n",
    "    train_labels_fold = [labels[i] for i in train_idx]\n",
    "    val_labels_fold = [labels[i] for i in val_idx]\n",
    "\n",
    "    train_loader = DataLoader(SoccerDataset(train_paths_fold, train_labels_fold, CONFIG), batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(SoccerDataset(val_paths_fold, val_labels_fold, CONFIG), batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "    model = R3DClassifier(num_classes=len(label_to_idx)).to(CONFIG['device'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if CONFIG['optimizer'] == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "    elif CONFIG['optimizer'] == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=CONFIG['learning_rate'], momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer in CONFIG.\")    \n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        model.train()\n",
    "        train_preds, train_targets = [], []\n",
    "\n",
    "        for videos, labels_batch in tqdm(train_loader, desc=f\"Fold {fold+1} - Epoch {epoch+1}\"):\n",
    "            videos, labels_batch = videos.to(CONFIG['device']), labels_batch.to(CONFIG['device'])\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_preds.extend(torch.argmax(outputs, 1).cpu().numpy())\n",
    "            train_targets.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(train_targets, train_preds)\n",
    "\n",
    "        model.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for videos, labels_batch in val_loader:\n",
    "                videos, labels_batch = videos.to(CONFIG['device']), labels_batch.to(CONFIG['device'])\n",
    "                outputs = model(videos)\n",
    "                val_preds.extend(torch.argmax(outputs, 1).cpu().numpy())\n",
    "                val_targets.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_targets, val_preds)\n",
    "        print(f\"‚úÖ Epoch {epoch+1}: Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            model_path = f\"../models/best_model_fold{fold+1}.pth\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"üíæ Saved best model for Fold {fold+1}: {model_path}\")\n",
    "\n",
    "    all_fold_accuracies.append(best_val_acc)\n",
    "    \n",
    "run_summary = {\n",
    "            'config': {\n",
    "                'learning_rate': CONFIG['learning_rate'],\n",
    "                'batch_size': CONFIG['batch_size'],\n",
    "                'dropout': CONFIG['dropout'],\n",
    "                'num_frames': CONFIG['num_frames'],\n",
    "                'optimizer': CONFIG['optimizer'],\n",
    "                'k_folds': CONFIG['k_folds']\n",
    "            },\n",
    "            'fold_accuracies': all_fold_accuracies,\n",
    "            'avg_accuracy': np.mean(all_fold_accuracies)\n",
    "        }\n",
    "\n",
    "with open(f\"run_result_lr{CONFIG['learning_rate']}.json\", \"w\") as f:\n",
    "    json.dump(run_summary, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar([f\"Fold {i+1}\" for i in range(k_folds)], all_fold_accuracies, color='skyblue')\n",
    "plt.axhline(np.mean(all_fold_accuracies), color='orange', linestyle='--', label='Average')\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Fold-wise Validation Accuracy\")\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
